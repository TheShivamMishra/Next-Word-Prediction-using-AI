{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Next_word_prediction_using_NGram.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPbsuKcBCEYYcp6yjQZjiYV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheShivamMishra/Next-Word-Prediction-using-AI/blob/main/Next_word_prediction_using_NGram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GndtBxet8u7_",
        "outputId": "d796702a-a701-49d2-8959-477d497af086"
      },
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict, Counter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from document1 import training_doc1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyaonWdu8vxG"
      },
      "source": [
        "class MarkovChain:\n",
        "  def __init__(self):\n",
        "    self.lookup_dict = defaultdict(list)  \n",
        "  \n",
        "  def add_document(self, string):\n",
        "    preprocessed_list = self._preprocess(string)\n",
        "    pairs = self.__generate_tuple_keys(preprocessed_list)\n",
        "    for pair in pairs:\n",
        "      self.lookup_dict[pair[0]].append(pair[1])\n",
        "    pairs2 = self.__generate_2tuple_keys(preprocessed_list)\n",
        "    for pair in pairs2:\n",
        "      self.lookup_dict[tuple([pair[0], pair[1]])].append(pair[2])\n",
        "    pairs3 = self.__generate_3tuple_keys(preprocessed_list)\n",
        "    for pair in pairs3:\n",
        "      self.lookup_dict[tuple([pair[0], pair[1], pair[2]])].append(pair[3])\n",
        "  \n",
        "  def _preprocess(self, string):\n",
        "    cleaned = re.sub(r'\\W+', ' ', string).lower()\n",
        "    tokenized = word_tokenize(cleaned)\n",
        "    return tokenized\n",
        "\n",
        "  def __generate_tuple_keys(self, data):\n",
        "    if len(data) < 1:\n",
        "      return\n",
        "\n",
        "    for i in range(len(data) - 1):\n",
        "      yield [ data[i], data[i + 1] ]\n",
        "  \n",
        "  def __generate_2tuple_keys(self, data):\n",
        "    if len(data) < 2:\n",
        "      return\n",
        "\n",
        "    for i in range(len(data) - 2):\n",
        "      yield [ data[i], data[i + 1], data[i+2] ]\n",
        "    \n",
        "    \n",
        "  def __generate_3tuple_keys(self, data):\n",
        "    if len(data) < 3:\n",
        "      return\n",
        "\n",
        "    for i in range(len(data) - 3):\n",
        "      yield [ data[i], data[i + 1], data[i+2], data[i+3] ]\n",
        "    \n",
        "  def oneword(self, string):\n",
        "    return Counter(self.lookup_dict[string]).most_common()[:3]\n",
        "\n",
        "  def twowords(self, string):\n",
        "        suggest = Counter(self.lookup_dict[tuple(string)]).most_common()[:3]\n",
        "        if len(suggest)==0:\n",
        "            return self.oneword(string[-1])\n",
        "        return suggest\n",
        "\n",
        "  def threewords(self, string):\n",
        "        suggest = Counter(self.lookup_dict[tuple(string)]).most_common()[:3]\n",
        "        if len(suggest)==0:\n",
        "            return self.twowords(string[-2:])\n",
        "        return suggest\n",
        "    \n",
        "  def morewords(self, string):\n",
        "        return self.threewords(string[-3:])\n",
        "\n",
        "    \n",
        "  def generate_text(self, string):\n",
        "    if len(self.lookup_dict) > 0:\n",
        "        tokens = string.split(\" \")\n",
        "        if len(tokens)==1:\n",
        "            print(\"Next word suggestions:\", self.oneword(string))\n",
        "        elif len(tokens)==2:\n",
        "            print(\"Next word suggestions:\", self.twowords(string.split(\" \")))\n",
        "        elif len(tokens)==3:\n",
        "            print(\"Next word suggestions:\", self.threewords(string.split(\" \")))\n",
        "        elif len(tokens)>3:\n",
        "            print(\"Next word suggestions:\", self.morewords(string.split(\" \")))\n",
        "    return"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uh-mMS1A-sW8",
        "outputId": "1fa009c0-58fa-4010-b38d-ee01a80f4e95"
      },
      "source": [
        "my_markov = MarkovChain()\n",
        "my_markov.add_document(training_doc1)\n",
        "my_markov.generate_text(input().lower())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hi\n",
            "Next word suggestions: [('lo', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZvW15OkCLJ8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}